#import pyrealsense2 as rs
import numpy as np
import cv2
import argparse
import time
import os
from math import atan2, cos, sin, sqrt, pi


def drawAxis(img, p_, q_, colour, scale):
    p = list(p_)
    q = list(q_)
    ## [visualization1]
    angle = atan2(p[1] - q[1], p[0] - q[0]) # angle in radians
    hypotenuse = sqrt((p[1] - q[1]) * (p[1] - q[1]) + (p[0] - q[0]) * (p[0] - q[0]))

    # Here we lengthen the arrow by a factor of scale
    q[0] = p[0] - scale * hypotenuse * cos(angle)
    q[1] = p[1] - scale * hypotenuse * sin(angle)
    cv2.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 1, cv2.LINE_AA)

    # create the arrow hooks
    p[0] = q[0] + 9 * cos(angle + pi / 4)
    p[1] = q[1] + 9 * sin(angle + pi / 4)
    cv2.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 1, cv2.LINE_AA)

    p[0] = q[0] + 9 * cos(angle - pi / 4)
    p[1] = q[1] + 9 * sin(angle - pi / 4)
    cv2.line(img, (int(p[0]), int(p[1])), (int(q[0]), int(q[1])), colour, 1, cv2.LINE_AA)
    ## [visualization1]

def getOrientation(pts, img):
    ## [pca]
    # Construct a buffer used by the pca analysis
    sz = len(pts)
    data_pts = np.empty((sz, 2), dtype=np.float64)
    for i in range(data_pts.shape[0]):
        data_pts[i,0] = pts[i,0,0]
        data_pts[i,1] = pts[i,0,1]

    # Perform PCA analysis
    mean = np.empty((0))
    mean, eigenvectors, eigenvalues = cv2.PCACompute2(data_pts, mean)

    # Store the center of the object
    cntr = (int(mean[0,0]), int(mean[0,1]))
    ## [pca]

    ## [visualization]
    # Draw the principal components
    cv2.circle(img, cntr, 3, (255, 0, 255), 2)
    p1 = (cntr[0] + 0.02 * eigenvectors[0,0] * eigenvalues[0,0], cntr[1] + 0.02 * eigenvectors[0,1] * eigenvalues[0,0])
    p2 = (cntr[0] - 0.02 * eigenvectors[1,0] * eigenvalues[1,0], cntr[1] - 0.02 * eigenvectors[1,1] * eigenvalues[1,0])
    drawAxis(img, cntr, p1, (0, 255, 0), 1)
    drawAxis(img, cntr, p2, (255, 255, 0), 5)

    angle = atan2(eigenvectors[0,1], eigenvectors[0,0]) # orientation in radians
    ## [visualization]

    return angle


ap = argparse.ArgumentParser()
ap.add_argument("-i", "--image", required=True,
	help="path to input image")

args = vars(ap.parse_args())

#load in our YOLO stuff

print("[INFO] loading YOLO from disk...")
net = cv2.dnn.readNetFromDarknet("cfg/yolov3.cfg", "weights/yolov3.weights")

#get all the classes
labelsPath = "data/coco.names"
LABELS = open(labelsPath).read().strip().split("\n")

#get indiv colours for each class
np.random.seed(42)
colors = np.random.uniform(0, 255, size=(len(LABELS), 3)) #make a colour for each class


layer_names = net.getLayerNames()
layer_names = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]


if args["image"] is not None: 
    color_image = cv2.imread(args["image"])
    color_image = cv2.resize(color_image,(640,480))
    (h, w, c) = color_image.shape

    blob = cv2.dnn.blobFromImage(color_image, 1/255, (416, 416),
            swapRB=True, crop=False) #make a blob

    net.setInput(blob)
    start = time.time()

    outs = net.forward(layer_names)
    end = time.time()

    print("[INFO] YOLO took {:.6f} seconds".format(end - start))


    class_ids = []
    confidences = []
    boxes = []
    # loop over the detections
    for out in outs:
        for detection in out:
            scores = detection[5:]
            class_id = np.argmax(scores)
            confidence = scores[class_id]

            if confidence > 0.7:
            #object detected!!!!
                box = detection[0:4] * np.array([w, h, w, h])
                (centerX, centerY, width, height) = box.astype("int")

                # Rectangle coordinates
                x = int(centerX - width / 2)
                y = int(centerY - height / 2)

                boxes.append([x, y, int(width), int(height)])
                confidences.append(float(confidence))
                class_ids.append(class_id)

    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)

    font = cv2.FONT_HERSHEY_PLAIN
    for i in range(len(boxes)):
        if i in indexes:
            x, y, w, h = boxes[i]
            label = str(LABELS[class_ids[i]])
            color = colors[i]
            cv2.rectangle(color_image, (x, y), (x + w, y + h), color, 2)
            text = "{}: {:.4f}".format(LABELS[class_ids[i]], confidences[i])

            cv2.putText(color_image, text, (x, y + 30), font, 3, color, 3)

    cv2.namedWindow('Image', cv2.WINDOW_AUTOSIZE)
    cv2.imshow('Image', color_image)

else: #if NO image has been parsed, we want to do webcam
    pipeline = rs.pipeline()
    config = rs.config()
    config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)
    # Start streaming
    pipeline.start(config)
    frames = pipeline.wait_for_frames()
    color_frame = frames.get_color_frame()
    color_image = np.asanyarray(color_frame.get_data())
    (h, w, c) = color_image.shape #get height width and channels

    try:
        while True:

            blob = cv2.dnn.blobFromImage(cv2.resize(color_image, (416, 416)), 0.007843, (416, 416), 127.5,
                swapRB=True, crop=False) #make a blob

            net.setInput(blob)
            start = time.time()

            outs = net.forward(output_layers)
            end = time.time()

            print("[INFO] YOLO took {:.6f} seconds".format(end - start))


            class_ids = []
            confidences = []
            boxes = []
            # loop over the detections
            for out in outs:
                for detection in out:
                    scores = detection[5:]
                    class_id = np.argmax(scores)
                    confidence = scores[class_id]

                    if confidence > 0.7:
                    #object detected!!!!
                        center_x = int(detection[0]*w)
                        center_y = int(detection[1]*h)
                        w = int(detection[2] * w)
                        h = int(detection[3] * h)

                        # Rectangle coordinates
                        x = int(center_x - w / 2)
                        y = int(center_y - h / 2)
                        boxes.append([x, y, w, h])
                        confidences.append(float(confidence))
                        class_ids.append(class_id)

            indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)

            print(indexes)
            font = cv2.FONT_HERSHEY_PLAIN
            for i in range(len(boxes)):
                if i in indexes:
                    x, y, w, h = boxes[i]
                    label = str(classes[class_ids[i]])
                    color = colors[i]
                    cv2.rectangle(color_image, (x, y), (x + w, y + h), color, 2)
                    cv2.putText(color_image, label, (x, y + 30), font, 3, color, 3)

            cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)
            cv2.imshow('RealSense', color_image)
            cv2.waitKey(0)

    finally:

        # Stop streaming
        pipeline.stop()

#------------------- NOW WE FIND ORIENTATION -------------------#
start = time.time()
src = cv2.imread(args["image"])
src = cv2.resize(src, (640, 480))

# Convert image to hsv
hsv = cv2.cvtColor(src, cv2.COLOR_BGR2HSV) 

#lets get some values for our hsv thresholds, what colours should we play between?

weaker = np.array([20, 100, 100]) #kind of a pale yellow colour
stronger = np.array([30, 255, 255]) #strong, vibrant yellow.

#make a mask

mask = cv2.inRange(hsv, weaker, stronger) 


## [contours]
# Find all the contours in the thresholded image
contours, _ = cv2.findContours(mask, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE) #mask to bw if using binary & grayscale

for i, c in enumerate(contours):
    # Calculate the area of each contour
    area = cv2.contourArea(c)
    # Ignore contours that are too small or too large
    if area < 1e2 or 1e5 < area:
        continue

    # Draw each contour only for visualisation purposes
    cv2.drawContours(src, contours, i, (0, 0, 255), 2)
    # Find the orientation of each shape
    angle = getOrientation(c, src)
    print("Orientation is", angle*(180/pi))
## [contours]

end = time.time()
print("[INFO] Orientation calc took {:.6f} seconds".format(end - start))

cv2.imshow('output', src)
cv2.waitKey()
cv2.destroyAllWindows()
